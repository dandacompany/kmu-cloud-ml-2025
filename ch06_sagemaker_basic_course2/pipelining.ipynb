{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 전처리 프로세싱 스크립트 (preprocess.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('script', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/preprocess.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_data(input_data_path, output_train_path, output_val_path, asset_path, n_components, test_size):\n",
    "    # 데이터 읽기\n",
    "    original_data = pd.read_csv(input_data_path)\n",
    "\n",
    "    # 특성과 타겟 분리\n",
    "    X = original_data.iloc[:, 1:]\n",
    "    y = original_data.iloc[:, 0]\n",
    "    \n",
    "    # 결측치 처리\n",
    "    X = X.replace('?', np.nan)\n",
    "    \n",
    "    # 타겟 변수 인코딩\n",
    "    if y.dtype == 'object':\n",
    "        y = y.map({\n",
    "            '<=50K': 0,\n",
    "            '<=50K.': 0,\n",
    "            '>50K': 1,\n",
    "            '>50K.': 1\n",
    "        })\n",
    "    else:\n",
    "        print(\"타겟 변수가 이미 숫자형입니다.\")\n",
    "\n",
    "    # 범주형 변수와 수치형 변수 구분\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # 범주형 변수에 'Unknown' 카테고리 추가 및 결측치 처리\n",
    "    for feature in categorical_features: \n",
    "        X[feature] = X[feature].astype('category')\n",
    "        X[feature] = X[feature].cat.add_categories('Unknown')\n",
    "        X[feature] = X[feature].fillna('Unknown')\n",
    "\n",
    "    # 수치형 특성의 결측치는 중앙값으로 대체\n",
    "    for feature in numeric_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median())\n",
    "    \n",
    "    # 훈련 - 검증 데이터셋 분할\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 2024)\n",
    "\n",
    "    # 데이터 스케일링\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_val[numeric_cols] = scaler.transform(X_val[numeric_cols])\n",
    "    \n",
    "    # 레이블 인코딩\n",
    "    encoders = {}\n",
    "    for col in categorical_features:\n",
    "        encoder = LabelEncoder()\n",
    "        X_train[col] = encoder.fit_transform(X_train[col])\n",
    "        encoders[col] = encoder\n",
    "        X_val[col] = encoder.transform(X_val[col])\n",
    "    \n",
    "\n",
    "    # PCA 수행\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pd.DataFrame(pca.fit_transform(X_train), index=X_train.index, columns=[f'PC{i}' for i in range(1, pca.n_components_+1)])\n",
    "    X_val_pca = pd.DataFrame(pca.transform(X_val), index=X_val.index, columns=[f'PC{i}' for i in range(1, pca.n_components_+1)])\n",
    "\n",
    "    \n",
    "    print(f\"훈련데이터 차원축소 : {X_train.shape} -> {X_train_pca.shape}\")\n",
    "    print(f\"검증데이터 차원축소 : {X_val.shape} -> {X_val_pca.shape}\")\n",
    "    \n",
    "    # 레이블 데이터 추가\n",
    "    train_data = pd.concat([y_train, X_train_pca], axis=1)\n",
    "    val_data = pd.concat([y_val, X_val_pca], axis=1)\n",
    "\n",
    "    # 전처리된 데이터 저장\n",
    "    train_data.to_csv(output_train_path, index=False)\n",
    "    val_data.to_csv(output_val_path, index=False)\n",
    "\n",
    "    # 인코더와 스케일러 저장\n",
    "    with open(os.path.join(asset_path, 'encoder.pkl'), 'wb') as f:\n",
    "        pickle.dump(encoders, f)\n",
    "    with open(os.path.join(asset_path, 'scaler.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(os.path.join(asset_path, 'pca.pkl'), 'wb') as f:\n",
    "        pickle.dump(pca, f)\n",
    "    \n",
    "    print(\"전처리 완료 및 데이터 저장 완료\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n-components', type=float, default=0.9)\n",
    "    parser.add_argument('--test-size', type=float, default=0.2)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    input_data_path = '/opt/ml/processing/input/original_data.csv'\n",
    "    output_train_path = '/opt/ml/processing/output/train/train_data.csv'\n",
    "    output_val_path = '/opt/ml/processing/output/validation/val_data.csv'\n",
    "    asset_path = '/opt/ml/processing/output/asset'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_train_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(output_val_path), exist_ok=True)\n",
    "    os.makedirs(asset_path, exist_ok=True)\n",
    "    \n",
    "    preprocess_data(input_data_path, output_train_path, output_val_path, asset_path, args.n_components, args.test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 훈련 스크립트 (train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/train.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import pickle as pkl\n",
    "from glob import glob\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # SageMaker 특정 인자 설정 (기본값은 환경 변수에서 가져옴)\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    # 하이퍼파라미터 설정\n",
    "    parser.add_argument('--max-depth', type=int, default=3)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    parser.add_argument('--reg-alpha', type=float, default=0)\n",
    "    parser.add_argument('--reg-lambda', type=float, default=1)\n",
    "    parser.add_argument('--subsample', type=float, default=1)\n",
    "    parser.add_argument('--colsample-bytree', type=float, default=1)\n",
    "    parser.add_argument('--num-round', type=int, default=200)\n",
    "    parser.add_argument('--early-stopping-rounds', type=int, default=10)\n",
    "    parser.add_argument('--objective', type=str, default='binary:logistic')\n",
    "    parser.add_argument('--eval-metric', type=str, default='auc')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # 데이터 로드\n",
    "    \n",
    "    # CSV 파일 목록 가져오기\n",
    "    train_files = glob(args.train + \"/*.csv\")\n",
    "    train_data = pd.concat([pd.read_csv(file) for file in train_files], ignore_index=True)\n",
    "    val_files = glob(args.validation + \"/*.csv\")\n",
    "    val_data = pd.concat([pd.read_csv(file) for file in val_files], ignore_index=True)\n",
    "\n",
    "    # 특성과 타겟 분리\n",
    "    X_train = train_data.iloc[:, 1:]\n",
    "    y_train = train_data.iloc[:, 0]\n",
    "    X_val = val_data.iloc[:, 1:]\n",
    "    y_val = val_data.iloc[:, 0]\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_val = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # XGBoost 모델 생성 및 훈련\n",
    "    watchlist = [(d_train, '훈련'), (d_val, '검증')]\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'reg_alpha': args.reg_alpha,\n",
    "        'reg_lambda': args.reg_lambda,\n",
    "        'subsample': args.subsample,\n",
    "        'colsample_bytree': args.colsample_bytree,\n",
    "        'objective': args.objective,\n",
    "        'eval_metric': args.eval_metric,\n",
    "    }\n",
    "    xgb_model = xgb.train(params, d_train, args.num_round, watchlist, early_stopping_rounds=args.early_stopping_rounds, verbose_eval=10)\n",
    "       \n",
    "    # 검증 데이터로 성능 평가\n",
    "    y_pred = xgb_model.predict(d_val)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred_binary)\n",
    "    precision = precision_score(y_val, y_pred_binary)\n",
    "    recall = recall_score(y_val, y_pred_binary)\n",
    "    f1 = f1_score(y_val, y_pred_binary)\n",
    "\n",
    "    print(f'검증 정확도: {accuracy:.4f}')\n",
    "    print(f'검증 정밀도: {precision:.4f}')\n",
    "    print(f'검증 재현율: {recall:.4f}')\n",
    "    print(f'검증 F1 점수: {f1:.4f}')\n",
    "\n",
    "    # 모델 저장\n",
    "    model_path = os.path.join(args.model_dir, 'xgboost-model')\n",
    "    pkl.dump(xgb_model, open(model_path, 'wb'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 평가 스크립트 (evaluate.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/evaluate.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle as pkl\n",
    "import tarfile\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def preprocess_test_data(test_data, assets):\n",
    "    \"\"\"입력 데이터를 전처리합니다.\"\"\"\n",
    "    scaler, encoders, pca = assets\n",
    "\n",
    "    y = test_data['income']\n",
    "    X = test_data.drop(columns=['income'])\n",
    "\n",
    "    # 결측치 처리\n",
    "    X = X.replace('?', np.nan)\n",
    "    \n",
    "    # 타겟 변수 인코딩\n",
    "    if y.dtype == 'object':\n",
    "        y = y.map({\n",
    "            '<=50K': 0,\n",
    "            '<=50K.': 0,\n",
    "            '>50K': 1,\n",
    "            '>50K.': 1\n",
    "        })\n",
    "    else:\n",
    "        print(\"타겟 변수가 이미 숫자형입니다.\")\n",
    "\n",
    "    # 범주형 변수와 수치형 변수 구분\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # 범주형 변수에 'Unknown' 카테고리 추가 및 결측치 처리\n",
    "    for feature in categorical_features: \n",
    "        X[feature] = X[feature].astype('category')\n",
    "        X[feature] = X[feature].cat.add_categories('Unknown')\n",
    "        X[feature] = X[feature].fillna('Unknown')\n",
    "\n",
    "    # 수치형 특성의 결측치는 중앙값으로 대체\n",
    "    for feature in numeric_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median())\n",
    "        \n",
    "    \n",
    "    # 범주형 컬럼 레이블 인코딩\n",
    "    for feature in encoders.keys() :\n",
    "        le = encoders[feature]\n",
    "        X[feature] = X[feature].astype(str)\n",
    "        # 인코더 업데이트\n",
    "        unique_values = np.unique(X[feature])\n",
    "        le.classes_ = np.unique(np.concatenate([le.classes_, unique_values]))\n",
    "        # 변환 처리\n",
    "        X[feature] = le.transform(X[feature])\n",
    "\n",
    "    # 스케일링\n",
    "    X[numeric_features] = scaler.transform(X[numeric_features])\n",
    "\n",
    "    # PCA 차원축소\n",
    "    X_pca = pca.transform(X)\n",
    "    X_pca = pd.DataFrame(X_pca, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])\n",
    "    \n",
    "    return X_pca, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 파일 로드\n",
    "    model_path = '/opt/ml/processing/model/model.tar.gz'\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    \n",
    "    xgb_model = pkl.load(open('xgboost-model', 'rb'))\n",
    "    \n",
    "    # S3에서 asset 파일을 로컬로 복사\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'dante-sagemaker' # 본인의 버킷명으로 반드시 수정하세요!\n",
    "    project_name = 'income-prediction'\n",
    "    \n",
    "    # 자산 파일 로드\n",
    "    scaler_key = f'{project_name}/asset/scaler.pkl'\n",
    "    encoder_key = f'{project_name}/asset/encoder.pkl'\n",
    "    pca_key = f'{project_name}/asset/pca.pkl'\n",
    "    \n",
    "    scaler_obj = s3.get_object(Bucket=bucket_name, Key=scaler_key)\n",
    "    encoder_obj = s3.get_object(Bucket=bucket_name, Key=encoder_key)\n",
    "    pca_obj = s3.get_object(Bucket=bucket_name, Key=pca_key)\n",
    "    \n",
    "    scaler = pkl.loads(scaler_obj['Body'].read())\n",
    "    encoders = pkl.loads(encoder_obj['Body'].read())\n",
    "    pca = pkl.loads(pca_obj['Body'].read())\n",
    "    \n",
    "    # 추론 데이터 로드\n",
    "    test_data = pd.read_csv('/opt/ml/processing/test/test.csv')\n",
    "    \n",
    "    # 추론 데이터 전처리\n",
    "    X, y_true = preprocess_test_data(test_data, (scaler, encoders, pca))\n",
    "    \n",
    "    # 추론 데이터 예측\n",
    "    dmatrix = xgb.DMatrix(X)\n",
    "    y_pred = xgb_model.predict(dmatrix)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # 평가 데이터 저장\n",
    "    report_dict = {\n",
    "        'classification_metrics': {\n",
    "            'roc_auc': roc_auc_score(y_true, y_pred_binary),\n",
    "            'accuracy': accuracy_score(y_true, y_pred_binary),\n",
    "            'precision': precision_score(y_true, y_pred_binary),\n",
    "            'recall': recall_score(y_true, y_pred_binary),\n",
    "            'f1': f1_score(y_true, y_pred_binary)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 추론 스크립트 (inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/inference.py\n",
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"XGBoost 모델과 필요한 자산을 `model_dir`에서 로드합니다.\"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    \n",
    "    # S3에서 asset 파일을 로컬로 복사\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'dante-sagemaker' # 본인의 버킷명으로 반드시 수정하세요!\n",
    "    project_name = 'income-prediction'\n",
    "    scaler_key = f'{project_name}/asset/scaler.pkl'\n",
    "    encoder_key = f'{project_name}/asset/encoder.pkl'\n",
    "    pca_key = f'{project_name}/asset/pca.pkl'\n",
    "    \n",
    "    scaler_obj = s3.get_object(Bucket=bucket_name, Key=scaler_key)\n",
    "    encoder_obj = s3.get_object(Bucket=bucket_name, Key=encoder_key)\n",
    "    pca_obj = s3.get_object(Bucket=bucket_name, Key=pca_key)\n",
    "    \n",
    "    scaler = pkl.loads(scaler_obj['Body'].read())\n",
    "    encoders = pkl.loads(encoder_obj['Body'].read())\n",
    "    pca = pkl.loads(pca_obj['Body'].read())\n",
    "    \n",
    "    original_feature_columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "       'capitalgain', 'capitalloss', 'hoursperweek', 'native-country']\n",
    "    \n",
    "    numeric_columns = ['age', 'fnlwgt', 'education-num', 'capitalgain', 'capitalloss', 'hoursperweek']\n",
    "    \n",
    "    return booster, (scaler, encoders, pca, original_feature_columns, numeric_columns)\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"입력 데이터 페이로드를 파싱합니다.\"\"\"\n",
    "    if request_content_type != \"text/csv\":\n",
    "        raise ValueError(f\"지원되지 않는 컨텐츠 타입입니다: {request_content_type}\")\n",
    "    df = pd.read_csv(io.StringIO(request_body), header=None)\n",
    "    return df.values\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"로드된 모델로 예측을 수행합니다.\"\"\"\n",
    "    booster, (scaler, encoders, pca, original_feature_columns, numeric_columns) = model\n",
    "    prep_input_data = preprocess_input_data(input_data, (scaler, encoders, pca, original_feature_columns, numeric_columns))\n",
    "    dmatrix = xgb.DMatrix(prep_input_data)\n",
    "    return booster.predict(dmatrix)\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"예측 출력을 포맷팅합니다.\"\"\"\n",
    "    if accept != \"text/csv\":\n",
    "        raise ValueError(f\"지원되지 않는 accept 타입입니다: {accept}\")\n",
    "    return ','.join(map(str, prediction))\n",
    "\n",
    "def preprocess_input_data(input_data, assets):\n",
    "    \"\"\"입력 데이터를 전처리합니다.\"\"\"\n",
    "    scaler, encoders, pca, original_feature_columns, numeric_columns = assets\n",
    "    X = pd.DataFrame(input_data, columns=original_feature_columns)\n",
    "    X[X == '?'] = np.nan\n",
    "\n",
    "    # 범주형 변수에 'Unknown' 카테고리 추가 및 결측치 처리\n",
    "    for feature in (set(original_feature_columns) - set(numeric_columns)): \n",
    "        X[feature] = X[feature].astype('category')\n",
    "        X[feature] = X[feature].cat.add_categories('Unknown')\n",
    "        X[feature] = X[feature].fillna('Unknown')\n",
    "\n",
    "    # 수치형 특성의 결측치는 중앙값으로 대체\n",
    "    for feature in set(numeric_columns):\n",
    "        X[feature] = pd.to_numeric(X[feature], errors='coerce')\n",
    "        X[feature] = X[feature].fillna(X[feature].median())\n",
    "    \n",
    "    X[numeric_columns] = X[numeric_columns].astype('float64')\n",
    "    X[numeric_columns] = scaler.transform(X[numeric_columns])\n",
    "    \n",
    "    # 범주형 컬럼 레이블 인코딩\n",
    "    for feature in encoders.keys() :\n",
    "        le = encoders[feature]\n",
    "        X[feature] = X[feature].astype(str)\n",
    "        # 인코더 업데이트\n",
    "        unique_values = np.unique(X[feature])\n",
    "        le.classes_ = np.unique(np.concatenate([le.classes_, unique_values]))\n",
    "        # 변환 처리\n",
    "        X[feature] = le.transform(X[feature])\n",
    "        \n",
    "    # NaN 및 무한대 값 처리\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # PCA 변환 수행\n",
    "    X_pca = pd.DataFrame(pca.transform(X), columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 유틸리티 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./../.env\") # 환경변수 파일의 상대적 위치를 확인하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 파이프라인 세션 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/dante/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto3_session = boto3.Session(profile_name='awstutor') # boto3 세션 생성\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3_session) # SageMaker 세션 생성\n",
    "pipeline_session = PipelineSession(boto_session=boto3_session) # 파이프라인 세션 생성\n",
    "role = os.getenv('SAGEMAKER_EXECUTION_ROLE_ARN') # 환경변수에서 역할 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 경로 설정 / 데이터 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'dante-sagemaker' # 본인의 버킷명으로 반드시 수정하세요!\n",
    "project_name = 'income-prediction'\n",
    "script_path = f's3://{bucket_name}/{project_name}/script'\n",
    "original_data_path = f's3://{bucket_name}/{project_name}/original_data'\n",
    "training_path = f's3://{bucket_name}/{project_name}/train'\n",
    "validation_path = f's3://{bucket_name}/{project_name}/val'\n",
    "test_path = f's3://{bucket_name}/{project_name}/test'\n",
    "evaluation_path = f's3://{bucket_name}/{project_name}/evaluation'\n",
    "model_path = f's3://{bucket_name}/{project_name}/model'\n",
    "asset_path = f's3://{bucket_name}/{project_name}/asset'\n",
    "batch_input_folder = f's3://{bucket_name}/{project_name}/batch/input'\n",
    "batch_output_folder = f's3://{bucket_name}/{project_name}/batch/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 삭제\n",
    "wr.s3.delete_objects(f\"s3://{bucket_name}/{project_name}/\", boto3_session=boto3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://dante-sagemaker/income-prediction/original_data/original_data.csv'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본 데이터 업로드\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Adult 데이터셋 로드\n",
    "adult = fetch_openml(name='adult', version=1, as_frame=True)\n",
    "X = adult.data\n",
    "y = adult.target\n",
    "y.name = 'income'\n",
    "\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size = 0.1, random_state = 2024)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size = 0.3, random_state = 2024)\n",
    "original_data = pd.concat([y_tmp, X_tmp], axis=1)\n",
    "wr.s3.to_csv(original_data, os.path.join(original_data_path, 'original_data.csv'), index=False, boto3_session=boto3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크립트 업로드\n",
    "wr.s3.upload(local_file='script/preprocess.py', path=os.path.join(script_path, 'preprocess.py'), boto3_session=boto3_session)\n",
    "wr.s3.upload(local_file='script/train.py', path=os.path.join(script_path, 'train.py'), boto3_session=boto3_session)\n",
    "wr.s3.upload(local_file='script/evaluate.py', path=os.path.join(script_path, 'evaluate.py'), boto3_session=boto3_session)\n",
    "wr.s3.upload(local_file='script/inference.py', path=os.path.join(script_path, 'inference.py'), boto3_session=boto3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://dante-sagemaker/income-prediction/batch/input/test.csv'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 업로드\n",
    "wr.s3.to_csv(pd.concat([y_test, X_test], axis=1), path=os.path.join(test_path, 'test.csv'), index=False, boto3_session=boto3_session) # 평가 조건용\n",
    "wr.s3.to_csv(X_test, path=os.path.join(batch_input_folder, 'test.csv'), index=False, boto3_session=boto3_session) # 배치 추론용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 전처리 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# SKLearnProcessor 인스턴스 생성\n",
    "# 데이터 전처리를 위한 SKLearn 프로세서를 설정합니다.\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',  # SKLearn 프레임워크 버전\n",
    "    role=role,  # 실행 역할\n",
    "    instance_type='ml.m5.xlarge',  # 사용할 인스턴스 유형\n",
    "    instance_count=1,  # 인스턴스 수\n",
    "    base_job_name=f'{project_name}-preprocessing',  # 기본 작업 이름\n",
    "    sagemaker_session=pipeline_session  # SageMaker 세션\n",
    ")\n",
    "\n",
    "# 프로세싱 작업 실행\n",
    "# 전처리 스크립트를 실행하고 입력/출력을 정의합니다.\n",
    "processor_args = sklearn_processor.run(\n",
    "    code=os.path.join(script_path, 'preprocess.py'),  # 실행할 전처리 스크립트 경로\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=os.path.join(original_data_path, 'original_data.csv'),  # 입력 데이터 소스\n",
    "            destination='/opt/ml/processing/input'  # 컨테이너 내 입력 데이터 위치\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train', source='/opt/ml/processing/output/train', destination=training_path),  # 훈련 데이터 출력\n",
    "        ProcessingOutput(output_name='validation', source='/opt/ml/processing/output/validation', destination=validation_path),  # 검증 데이터 출력\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test/\", destination=test_path),  # 테스트 데이터 출력\n",
    "        ProcessingOutput(output_name='asset', source='/opt/ml/processing/output/asset', destination=asset_path)  # 기타 자산 출력\n",
    "    ],\n",
    "    arguments=['--n-components', '0.9', '--test-size', '0.2']  # 전처리 스크립트에 전달할 인자\n",
    ")\n",
    "\n",
    "# 전처리 단계를 파이프라인 스텝으로 정의\n",
    "preprocess_step = ProcessingStep(name=\"PreprocessingStep\", step_args=processor_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 훈련 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "# XGBoost 모델의 하이퍼파라미터를 정의합니다.\n",
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",  # 트리의 최대 깊이\n",
    "    \"eta\": \"0.2\",  # 학습률\n",
    "    \"gamma\": \"4\",  # 트리의 리프 노드를 추가적으로 나누기 위한 최소 손실 감소\n",
    "    \"min_child_weight\": \"6\",  # 자식 노드에 필요한 최소 가중치 합\n",
    "    \"subsample\": \"0.7\",  # 각 트리마다 사용할 훈련 데이터의 샘플링 비율\n",
    "    \"objective\": \"binary:logistic\",  # 이진 분류를 위한 목적 함수\n",
    "    \"num_round\": \"200\",  # 부스팅 라운드 수\n",
    "    \"early_stopping_rounds\": \"10\",  # 조기 종료를 위한 라운드 수\n",
    "    \"eval_metric\": \"logloss\",  # 평가 지표\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델 객체 생성\n",
    "xgb_model = XGBoost(\n",
    "    role=role,  # IAM 역할\n",
    "    entry_point=\"script/train.py\",  # 훈련 스크립트 경로\n",
    "    framework_version=\"1.7-1\",  # XGBoost 프레임워크 버전\n",
    "    output_path=model_path,  # 모델 출력 경로\n",
    "    sagemaker_session=pipeline_session,  # SageMaker 세션\n",
    "    instance_count=1,  # 사용할 인스턴스 수\n",
    "    instance_type='ml.m5.xlarge',  # 인스턴스 유형\n",
    "    base_job_name=f\"{project_name}-xgboost\",  # 기본 작업 이름\n",
    "    max_run=60*60,  # 최대 실행 시간 (1시간)\n",
    "    max_wait=60*60,  # 최대 대기 시간 (1시간)\n",
    "    use_spot_instances=True,  # 스팟 인스턴스 사용\n",
    "    hyperparameters=hyperparams,  # 하이퍼파라미터\n",
    "    code_location=script_path  # 스크립트 위치를 명시적으로 지정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델 훈련 실행\n",
    "train_args = xgb_model.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri, \n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri, \n",
    "            content_type='text/csv'\n",
    "        ),        \n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True,\n",
    "    job_name=name_from_base(f\"{project_name}-xgboost-training\"),\n",
    ")\n",
    "\n",
    "# 훈련 단계 정의\n",
    "train_step = TrainingStep(\n",
    "    name='TrainingStep',  # 훈련 단계의 이름\n",
    "    step_args=train_args,  # 훈련 인자\n",
    "    cache_config=False  # 캐시 설정 비활성화\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 평가 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker 처리 및 워크플로우 관련 모듈 임포트\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# XGBoost 이미지 URI 검색\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region='ap-northeast-2',\n",
    "    version=\"1.7-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "# 모델 평가를 위한 ScriptProcessor 객체 생성\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{project_name}-evaluation\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "# 평가 스크립트 실행을 위한 인자 설정\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        # 훈련된 모델 아티팩트 입력\n",
    "        ProcessingInput(\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        # 테스트 데이터셋 입력\n",
    "        ProcessingInput(\n",
    "            source=preprocess_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        # 평가 결과 출력 설정\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\", destination=evaluation_path),\n",
    "    ],\n",
    "    code=\"script/evaluate.py\",  # 평가 스크립트 경로\n",
    ")\n",
    "\n",
    "# 평가 보고서 파일 속성 정의\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "# 평가 단계 정의\n",
    "eval_step = ProcessingStep(\n",
    "    name=\"EvaluationStep\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 추론 모델 생성 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker.workflow.model_step import ModelStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델 객체 생성\n",
    "xgb_inf_model = XGBoostModel(\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    entry_point=\"script/inference.py\",\n",
    "    code_location=script_path,\n",
    "    framework_version=\"1.7-1\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = xgb_inf_model.create(instance_type=\"ml.m5.xlarge\")  # XGBoost 모델 생성 인자 설정 (인스턴스 타입: ml.m5.xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델 생성 단계 정의\n",
    "create_model_step = ModelStep(\n",
    "   name=\"XGBCreation\",  # 단계 이름 설정\n",
    "   step_args=model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 등록 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.t2.medium.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "# 모델 그룹 이름 정의\n",
    "model_group_name = \"AdultIncomePredictionXGBModelPackage\"\n",
    "\n",
    "# 모델 메트릭스 객체 생성\n",
    "# 평가 단계에서 생성된 evaluation.json 파일의 S3 URI를 사용하여 모델 통계 설정\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            eval_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# XGBoost 추론 모델 등록을 위한 인자 설정\n",
    "register_args = xgb_inf_model.register(\n",
    "    content_types=[\"text/csv\"],  # 입력 데이터 타입\n",
    "    response_types=[\"text/csv\"],  # 출력 데이터 타입\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],  # 추론에 사용할 인스턴스 유형\n",
    "    transform_instances=[\"ml.m5.xlarge\"],  # 배치 변환에 사용할 인스턴스 유형\n",
    "    model_package_group_name=model_group_name,  # 모델 그룹 이름\n",
    "    approval_status='PendingManualApproval',  # 모델 승인 상태 (수동 승인 대기)\n",
    "    model_metrics=model_metrics,  # 모델 메트릭스\n",
    ")\n",
    "\n",
    "# 모델 등록 단계 정의\n",
    "register_step = ModelStep(name=\"XGBRegistration\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 배치 변환 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Transformer와 TransformStep을 가져옵니다\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "# 배치 변환을 위한 Transformer 객체를 생성합니다\n",
    "transformer = Transformer(\n",
    "    model_name = create_model_step.properties.ModelName,  # 생성된 모델의 이름을 사용합니다\n",
    "    instance_count = 1,  # 변환에 사용할 인스턴스 수\n",
    "    instance_type = 'ml.m4.xlarge',  # 변환에 사용할 인스턴스 유형\n",
    "    strategy = 'MultiRecord',  # 여러 레코드를 한 번에 처리하는 전략\n",
    "    assemble_with = 'Line',  # 출력을 줄 단위로 조립합니다\n",
    "    output_path = batch_output_folder,  # 변환 결과의 출력 경로\n",
    "    base_transform_job_name=f'{project_name}-inference-batch',  # 변환 작업의 기본 이름\n",
    "    sagemaker_session=pipeline_session,  # SageMaker 세션\n",
    "    accept = 'text/csv'  # 출력 데이터 형식\n",
    ")\n",
    "\n",
    "# 배치 변환을 위한 인자 설정\n",
    "tranaformer_args = transformer.transform(\n",
    "    data=batch_input_folder,  # 배치 입력 데이터 위치\n",
    "    content_type='text/csv'  # 입력 데이터 형식\n",
    ")\n",
    "\n",
    "# 배치 변환 단계 정의\n",
    "transforming_step = TransformStep(\n",
    "    name=\"TransformStep\",  # 단계 이름\n",
    "    step_args=tranaformer_args,  # 변환 인자\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 실패 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker 워크플로우 라이브러리에서 필요한 클래스들을 가져옵니다\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.parameters import ParameterFloat\n",
    "\n",
    "# ROAUC 임계값을 파라미터로 정의합니다\n",
    "roauc_threshold = ParameterFloat(name=\"ROAUCThreshold\", default_value=0.65) \n",
    "\n",
    "# 실패 단계를 정의합니다\n",
    "fail_step = FailStep(\n",
    "    name=\"FailStep\",  # 실패 단계의 이름\n",
    "    error_message=Join(on=\" \", values=[\"목표 모델생성에 실패했습니다. AUROC <\", roauc_threshold]),  # 오류 메시지 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 조건 스텝 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker 워크플로우 라이브러리에서 필요한 클래스들을 가져옵니다\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThan\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "# ROC AUC 점수가 임계값보다 큰지 확인하는 조건을 정의합니다\n",
    "cond_gt = ConditionGreaterThan(\n",
    "    left=JsonGet(\n",
    "        step_name=eval_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.roc_auc\",\n",
    "    ),\n",
    "    right=roauc_threshold,\n",
    ")\n",
    "\n",
    "# 조건에 따라 다른 단계를 실행하는 조건 단계를 정의합니다\n",
    "condition_step = ConditionStep(\n",
    "    name=\"ConditionStep\",  # 조건 단계의 이름\n",
    "    conditions=[cond_gt],  # 평가할 조건\n",
    "    if_steps=[create_model_step, register_step, transforming_step],  # 조건이 참일 때 실행할 단계들\n",
    "    else_steps=[fail_step],  # 조건이 거짓일 때 실행할 단계\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 파이프라인 정의 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker 워크플로우 파이프라인을 위한 라이브러리 임포트\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# 파이프라인 이름 설정\n",
    "pipeline_name = \"AdultIncomePredictionPipeline\"\n",
    "\n",
    "# 처리 및 훈련에 사용할 인스턴스 유형 및 개수 설정\n",
    "processing_instance_type = 'ml.m5.xlarge'\n",
    "processing_instance_count = 1\n",
    "training_instance_type = 'ml.m5.xlarge'\n",
    "\n",
    "# 모델 승인 상태 설정\n",
    "model_approval_status = 'PendingManualApproval'\n",
    "\n",
    "# 입력 데이터 및 배치 데이터 경로 설정\n",
    "input_data = original_data_path\n",
    "batch_data = batch_input_folder\n",
    "\n",
    "# 파이프라인 객체 생성\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,  # 파이프라인 이름\n",
    "    parameters=[  # 파이프라인 파라미터 설정\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        roauc_threshold\n",
    "    ],\n",
    "    sagemaker_session=pipeline_session,  # 파이프라인 세션\n",
    "    steps=[preprocess_step, train_step, eval_step, condition_step],  # 파이프라인 단계 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:905418381372:pipeline/AdultIncomePredictionPipeline',\n",
       " 'ResponseMetadata': {'RequestId': '95cd1ab7-84ca-48c1-9687-267f5bd78ac4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '95cd1ab7-84ca-48c1-9687-267f5bd78ac4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '102',\n",
       "   'date': 'Mon, 21 Oct 2024 15:35:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인을 생성하거나 업데이트합니다.\n",
    "# role_arn 파라미터는 파이프라인 실행에 필요한 IAM 역할을 지정합니다.\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 실행 시작\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:905418381372:pipeline/AdultIncomePredictionPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-northeast-2:905418381372:pipeline/AdultIncomePredictionPipeline/execution/o0kkgc6z2fnk',\n",
       " 'PipelineExecutionDisplayName': 'execution-1729524908760',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2024, 10, 22, 0, 35, 8, 696000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 10, 22, 0, 35, 8, 696000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:iam::905418381372:user/tutor_sdk',\n",
       "   'PrincipalId': 'AIDA5FTZEDA6GVWMFH7I2'}},\n",
       " 'LastModifiedBy': {'IamIdentity': {'Arn': 'arn:aws:iam::905418381372:user/tutor_sdk',\n",
       "   'PrincipalId': 'AIDA5FTZEDA6GVWMFH7I2'}},\n",
       " 'ResponseMetadata': {'RequestId': '11c6d65a-0a02-40bb-a2b9-a282107676ec',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '11c6d65a-0a02-40bb-a2b9-a282107676ec',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '637',\n",
       "   'date': 'Mon, 21 Oct 2024 15:35:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인 실행 상태 및 세부 정보를 조회합니다.\n",
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 실행이 완료될 때까지 대기합니다.\n",
    "# 이 함수는 파이프라인 실행이 끝날 때까지 프로그램의 실행을 일시 중지시킵니다.\n",
    "execution.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
